{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76096ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import dolfin as dl\n",
    "# # from hippylib import nb\n",
    "# from fenics import *\n",
    "\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.getLogger('FFC').setLevel(logging.WARNING)\n",
    "logging.getLogger('UFL').setLevel(logging.WARNING)\n",
    "# dl.set_log_active(False)\n",
    "\n",
    "# np.random.seed(seed=1)\n",
    "\n",
    "# modify code -----1\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "### Import necessary modules and setup the environment\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import normal\n",
    "import torch.optim as optim\n",
    "from scipy import ndimage\n",
    "from scipy.fftpack import dct, idct\n",
    "import random\n",
    "import time\n",
    "import tracemalloc\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# from google.colab import files\n",
    "\n",
    "\n",
    "from matplotlib import image\n",
    "from matplotlib import pyplot\n",
    "from numpy import exp,arange\n",
    "from pylab import meshgrid,cm,imshow,contour,clabel,colorbar,axis,title,show\n",
    "\n",
    "from sksparse.cholmod import cholesky\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def mainformula(x,y):\n",
    "    z = -2* y + 1.6\n",
    "\n",
    "    # condition1 = (y > .5)\n",
    "    # z[condition1] = 1\n",
    "\n",
    "    # condition2 = (x < 0.6) & (x > 0.3) & (y < 0.9) & (y > 0.6)\n",
    "    # z[condition2] = 0 # np.log(4)\n",
    "\n",
    "    condition3 = (x<0.4) & (x > 0.1) & (y<0.8) & (y > 0.2)\n",
    "    m, n = condition3.shape\n",
    "    condition1 = x < 2\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            condition1[i, j] = not condition1[i, j]\n",
    "            if (not condition3[i, j]) & (i <= m//2):\n",
    "                condition1[i, j] = True\n",
    "            condition3[i, j] = not condition3[i, j]\n",
    "    z[condition3] = 1. # - y + 1.3\n",
    "\n",
    "    condition2 = (x < 0.9) & (x > 0.45) & (y < 0.9) & (y > 0.6)\n",
    "    z[condition2] = 0 # np.log(4)\n",
    "    z[condition1] = 0.5 # np.log(4)\n",
    "\n",
    "\n",
    "#     condition2 = ( x-.75)** 2 + (y -.75) ** 2 < .1 ** 2\n",
    "#     z[condition2] = 0.5\n",
    "\n",
    "\n",
    "\n",
    "    return z\n",
    "\n",
    "def activation(z):\n",
    "    return torch.tanh(z)\n",
    "\n",
    "# activation = nn.ReLU()\n",
    "\n",
    "\n",
    "M = 100\n",
    "(x0, xf) = (0, 1.0)\n",
    "(y0, yf) = (0, 1.0)\n",
    "\n",
    "hx = (xf - x0)/(M-1)\n",
    "hy = (yf - y0)/(M-1)\n",
    "\n",
    "X, Y = np.meshgrid(np.linspace(x0, xf, M), np.linspace(y0, yf, M))\n",
    "# Z = z_func(X, Y) # evaluation of the function on the grid\n",
    "Z = mainformula(X, Y).T\n",
    "\n",
    "c = plt.imshow(Z)\n",
    "plt.colorbar(c)\n",
    "plt.title('true image')\n",
    "plt.show()\n",
    "\n",
    "def LHS(M):\n",
    "\n",
    "    N = M**2-1\n",
    "\n",
    "    main_diag = 4*np.ones((M, 1)).ravel()\n",
    "    off_diag = -1*np.ones((M-1, 1)).ravel()\n",
    "\n",
    "    a = main_diag.shape[0]\n",
    "\n",
    "    diagonals = [main_diag, off_diag, off_diag]\n",
    "\n",
    "    B = sparse.diags(diagonals, [0,-1,1], shape=(a,a)).toarray()\n",
    "    B[0, 0] = 3\n",
    "    B[0,1] = -1.0\n",
    "    B[M-1,M-2] = -1.0\n",
    "    B[M-1,M-1] = 3\n",
    "\n",
    "    D = sparse.diags([-1*np.ones((M+1, 1)).ravel()], [0], shape=(a,a)).toarray()\n",
    "\n",
    "    C = sparse.diags([-1*np.ones((M+1, 1)).ravel()], [0], shape=(a,a)).toarray()\n",
    "\n",
    "    e1 = sparse.eye(M).toarray()\n",
    "    A1 = sparse.kron(e1,B).toarray()\n",
    "\n",
    "    e2 = sparse.diags([1*np.ones((M, 1)).ravel(),1*np.ones((M, 1)).ravel()], [-1,1], shape=(M,M)).toarray()\n",
    "    e2[0,1] = 0.0\n",
    "    e2[M-1,M-2] = 0.0\n",
    "    A2 = sparse.kron(e2,D).toarray()\n",
    "\n",
    "    e3 = sparse.diags([1*np.ones((M, 1)).ravel(),1*np.ones((M, 1)).ravel()], [-1,1], shape=(M,M)).toarray()\n",
    "    e3[1:M-1,0:M] = 0.0\n",
    "    A3 = sparse.kron(e3,C).toarray()\n",
    "\n",
    "    mat = A1 + A2 + A3\n",
    "\n",
    "    for i in range(M):\n",
    "        mat[i, i] -= 1\n",
    "        mat[N -i, N -i] -= 1\n",
    "\n",
    "    return mat\n",
    "\n",
    "N = M ** 2\n",
    "N_obs = 2*10**2\n",
    "\n",
    "#obsInd = np.random.choice((N-1), N_obs, replace = False)\n",
    "obsStep = np.round(M/(np.sqrt(N_obs)+1)).astype(int)\n",
    "obsBase = np.arange(obsStep,np.sqrt(N_obs)*obsStep+1,obsStep).astype(int)-1\n",
    "obsInd = obsBase + (obsStep-1)*M\n",
    "\n",
    "for p in np.arange(1,np.sqrt(N_obs)).astype(int):\n",
    "    obsInd = np.append(obsInd,obsBase + (obsStep*p-1)*M)\n",
    "\n",
    "kappa = 0.01\n",
    "A = np.identity(M ** 2) + M**2 * kappa * LHS(M)\n",
    "\n",
    "u_t = np.linalg.solve(A, Z.reshape(-1))\n",
    "\n",
    "obsX = [index%M for index in obsInd]\n",
    "obsY = [M-1 - int(index/M) for index in obsInd]\n",
    "\n",
    "\n",
    "noise_scale = .01\n",
    "\n",
    "u_obs = u_t.reshape(-1)[obsInd]\n",
    "u_obs += noise_scale * np.random.normal(0, 1, u_obs.shape)\n",
    "\n",
    "D_in = 2\n",
    "H1 = 80\n",
    "H2 = 80\n",
    "H3 = 1000\n",
    "D_out = 1\n",
    "\n",
    "P = np.zeros((1, N))\n",
    "for i in obsInd:\n",
    "    res = np.zeros((1, N));\n",
    "    res[0, i] = 1\n",
    "    P = np.concatenate((P, res), axis = 0)\n",
    "\n",
    "\n",
    "P = P[1:, :]\n",
    "\n",
    "P_T = P.T\n",
    "A_T = A.T\n",
    "\n",
    "factor1 = cholesky(csr_matrix(A))\n",
    "factor2 = cholesky(csr_matrix(A_T))\n",
    "\n",
    "m_normal = normal.Normal(0, 1)\n",
    "class Cauchy(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        Forward equation\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return torch.tan(np.pi*m_normal.cdf(input)-np.pi/2) # input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Backward equation for auto-computing derivatives\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input = grad_input*((1 + torch.tan(np.pi*m_normal.cdf(input)-np.pi/2).pow(2)))*(np.pi*torch.exp(m_normal.log_prob(input)))\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "cauchy = Cauchy.apply\n",
    "\n",
    "## ============ Customized Neural Network =======\n",
    "class CauchyNetwork(nn.Module):\n",
    "    def __init__(self, D_in = 2, H1 = 200, H2 = 60, H3 = 160, D_out = 1):\n",
    "        super().__init__()\n",
    "#         torch.manual_seed(1)\n",
    "\n",
    "        self.w1 = nn.Parameter(torch.randn((H1, D_in),dtype=torch.double))\n",
    "        self.b1 = nn.Parameter(torch.randn((H1, 1),dtype=torch.double))\n",
    "        self.w2 = nn.Parameter(torch.randn((H2, H1),dtype=torch.double))\n",
    "        self.b2 = nn.Parameter(torch.randn((H2, 1),dtype=torch.double))\n",
    "        self.w3 = nn.Parameter(torch.randn((H3, H2),dtype=torch.double))\n",
    "        self.b3 = nn.Parameter(torch.randn((H3, 1),dtype=torch.double))\n",
    "        self.w4 = nn.Parameter(torch.randn((D_out, H3),dtype=torch.double))\n",
    "\n",
    "    def forward(self, X):\n",
    "        hidden_1 = activation(self.w1.mm(X) + self.b1)\n",
    "        hidden_2 = activation(self.w2.mm(hidden_1) + self.b2)\n",
    "        hidden_3 = activation(self.w3.mm(hidden_2) + self.b3)\n",
    "        y_pred = self.w4.mm(hidden_3)/list(self.w4.shape)[-1]\n",
    "\n",
    "#         hidden_1 = torch.tanh(cauchy(self.w1).mm(XX) + cauchy(self.b1))\n",
    "#         hidden_2 = torch.tanh(cauchy(self.w2).mm(hidden_1) + cauchy(self.b2))\n",
    "#         hidden_3 = torch.tanh(cauchy(self.w3).mm(hidden_2) + cauchy(self.b3))\n",
    "#         y_pred = self.w4.mm(hidden_3)/np.sqrt(40)\n",
    "        return y_pred\n",
    "\n",
    "class PDEsolver(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, ud):\n",
    "        \"\"\"\n",
    "        Forward equation\n",
    "        \"\"\"\n",
    "        m_numpy = input.detach().numpy().reshape(-1)\n",
    "        u_numpy = factor1(m_numpy)\n",
    "        res = P.dot(u_numpy) - ud\n",
    "        g = factor2(P_T.dot(res))\n",
    "\n",
    "        ctx.save_for_backward(torch.tensor(g, dtype = torch.double).view(1, -1))\n",
    "\n",
    "        return torch.tensor(.5 * res.dot(res), dtype = torch.double)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        g, = ctx.saved_tensors\n",
    "        return g, None\n",
    "\n",
    "XX = torch.tensor(np.concatenate((X.reshape(1, -1), Y.reshape(1, -1)), axis = 0), dtype = torch.double)\n",
    "\n",
    "\n",
    "\n",
    "m_v1 = []\n",
    "for ou_i in range(1):\n",
    "    learning_rate = .01\n",
    "\n",
    "\n",
    "    # torch.manual_seed(1)\n",
    "    e = 1\n",
    "    # gamma = 20\n",
    "\n",
    "    gm = noise_scale\n",
    "\n",
    "    reg_para =  gm ** 2\n",
    "\n",
    "\n",
    "\n",
    "    # net = GaussianNetwork()\n",
    "    net = CauchyNetwork(D_in = D_in, H1 = H1, H2 = H2, H3 = H3, D_out = D_out)\n",
    "\n",
    "    solver = PDEsolver.apply\n",
    "\n",
    "\n",
    "\n",
    "#     loss_fn = torch.nn.MSELoss(reduction='sum', size_average=False)\n",
    "\n",
    "    adam_steps = 500\n",
    "    lbfgs_steps = adam_steps+1500\n",
    "\n",
    "    loss_history = []\n",
    "    grad_history = []\n",
    "    reg_history = []\n",
    "    total_history = []\n",
    "\n",
    "\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # for k in range(adam_steps):\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(net.parameters(), lr=learning_rate,line_search_fn='strong_wolfe')\n",
    "\n",
    "\n",
    "    # print('LBFGS')\n",
    "\n",
    "\n",
    "    for k in range(lbfgs_steps):\n",
    "        net.train()\n",
    "        if k < adam_steps:\n",
    "            optt = opt\n",
    "        else:\n",
    "            optt = optimizer\n",
    "        if k == adam_steps:\n",
    "            print('LBFGS')\n",
    "        def closure():\n",
    "            optt.zero_grad()\n",
    "            # loss = neg_log_posterior(net, x_obs_torch, y_obs_torch, std_noise)\n",
    "            y_pred = net(XX)\n",
    "            loss = solver(y_pred, u_obs)\n",
    "#             print(loss)\n",
    "            loss_l = loss.detach().item()\n",
    "            reg_loss = 0\n",
    "            for name, param in net.named_parameters():\n",
    "                if name == 'w4':\n",
    "                    reg_loss += param.pow(2).sum()\n",
    "                else:\n",
    "                    reg_loss += torch.log(1 + param.pow(2)).sum()\n",
    "\n",
    "            loss += reg_para * reg_loss\n",
    "            loss.backward()\n",
    "            gr = 0\n",
    "            for param in net.parameters():\n",
    "                gr += param.grad.pow(2).mean().detach().numpy()/7\n",
    "    #         for name, param in net.named_parameters():\n",
    "    #             print(name, param.grad.pow(2).sum())\n",
    "            loss_history.append( loss_l)\n",
    "            grad_history.append(gr)\n",
    "            total_history.append(loss.detach().item())\n",
    "            reg_history.append((reg_para*reg_loss).detach().item())\n",
    "\n",
    "            return loss\n",
    "        # optimizer.zero_grad()\n",
    "        optt.step(closure)\n",
    "        if k % 20 == 0:\n",
    "            print(k)\n",
    "            y_test = net(XX)\n",
    "            m_array = y_test.detach().numpy()\n",
    "            c = plt.imshow(m_array.reshape(M, M))\n",
    "            plt.show()\n",
    "            plt.colorbar(c)\n",
    "\n",
    "#             print(k, loss_l, (reg_para*reg_loss).detach().item(), gr)\n",
    "    dic2 = {}\n",
    "    for name, param in net.named_parameters():\n",
    "        dic2[str(name)] = param.detach()\n",
    "\n",
    "    hidden_1 = activation(dic2['w1'].mm(XX) + dic2['b1'])\n",
    "    hidden_2 = activation(dic2['w2'].mm(hidden_1) + dic2['b2'])\n",
    "    hidden_3 = activation(dic2['w3'].mm(hidden_2) + dic2['b3'])\n",
    "    y_pred = dic2['w4'].mm(hidden_3)/H3\n",
    "    m_v1.append(y_pred)\n",
    "\n",
    "#     hidden_1 = torch.tanh(cauchy(dic1['w1']).mm(XX) + cauchy(dic1['b1']))\n",
    "#     hidden_2 = torch.tanh(cauchy(dic1['w2']).mm(hidden_1) + cauchy(dic1['b2']))\n",
    "#     hidden_3 = torch.tanh(cauchy(dic1['w3']).mm(hidden_2) + cauchy(dic1['b3']))\n",
    "#     y_pred = dic1['w4'].mm(hidden_3)/np.sqrt(40)\n",
    "#     m_v1.append(y_pred)\n",
    "\n",
    "    if ou_i == 0:\n",
    "        zzz = hidden_3.detach().numpy()\n",
    "    else:\n",
    "        zzz = np.concatenate((zzz, hidden_3.detach().numpy()))\n",
    "\n",
    "# c = plt.imshow(m_array.reshape(M, M))\n",
    "# plt.colorbar(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b79fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_res = zzz.copy()\n",
    "nbase = 1000\n",
    "gmmm = noise_scale\n",
    "basis = bs_res\n",
    "Q = P.dot(factor1(basis.T))   #/np.sqrt(nbase)\n",
    "sigma_Q = np.linalg.inv(1/(gmmm**2) * (Q.T).dot(Q) + 1/1*np.eye(nbase))\n",
    "MM = sigma_Q.dot(Q.T.dot(u_obs))\n",
    "Mean_reg = basis.T.dot(MM)/gmmm**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e6eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
    "fig=plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "c = plt.imshow(Mean_reg.reshape(M, M), vmin = 0, vmax = 1.2, cmap = 'rainbow')\n",
    "plt.axis('off')\n",
    "cax = fig.add_axes([ax.get_position().x1+0.01,ax.get_position().y0,0.02,ax.get_position().height])\n",
    "plt.colorbar(c, cax = cax)\n",
    "plt.savefig('GRcauchymean.pdf', bbox_inches = \"tight\")\n",
    "# plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
